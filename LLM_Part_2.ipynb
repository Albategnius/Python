{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSJrqeEOZfZ7m5+AY16qGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Albategnius/Python/blob/master/LLM_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@raufpokemon00/building-a-large-language-model-llm-from-scratch-61fed0570ea5#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjA3YjgwYTM2NTQyODUyNWY4YmY3Y2QwODQ2ZDc0YThlZTRlZjM2MjUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDg0MDg4MTA4OTMzMTY4MTQ0NjgiLCJlbWFpbCI6ImhhcmRpcm9iYnlAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTc0NjQ4NjQ1NSwibmFtZSI6IlJvYmJ5IEhhcmRpIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FDZzhvY0s5Wk96TFJnbmtYNFZvWWR3Y1U2THlYX3MzM1g5eEpaTi1sUXFTbFloYzFaZDR5Y0ZvPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6IlJvYmJ5IiwiZmFtaWx5X25hbWUiOiJIYXJkaSIsImlhdCI6MTc0NjQ4Njc1NSwiZXhwIjoxNzQ2NDkwMzU1LCJqdGkiOiIyOGY0MzI0MjViZTE3ODlkZTFiNDVmN2MwN2MwMGExMjhkYjkyYTRhIn0.iuKE2lPMv7KZmgqDF6YXKDl334mJJD9eMlvI-Pbv35YfXnTsyGNBywggr6_jSlFpPJUwQDn8PQVRUdKRcC_5nrB_HyuejXxWqxzKQg4aqGLNHQO6agDDoM-2XBEzYNVHjY2uqVL1YN1ULw-TGvyqH0hJnpT254C-E6Mher60FVoqwQZVybfA6zlRCRaFG3YsIU_xJlmnoN9njCDgmXZJdCC9S3C8d_okyoIsT_2tWWmGG02WzMBkFZLA_PjzhI9IH3BgqoH-1WheJGoO_zm3YrTRdtXz0vU61WS1J20sHPzY4Bd9cD85VglOw2_YthZxHrR-L8bGhpuPdjmLonIbKg"
      ],
      "metadata": {
        "id": "y3SBggfk9xrZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kTVXooON9lnJ"
      },
      "outputs": [],
      "source": [
        "# Scrapping\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: Specify the URL\n",
        "url = \"https://wikipedia.com\"\n",
        "\n",
        "# Step 2: Send a GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 3: Parse the website content\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Step 4: Extract all text from the page\n",
        "text_data = soup.get_text()\n",
        "\n",
        "# Step 5: Print the first 500 characters of the text\n",
        "print(text_data[:500])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"My name is Abdul Rauf jatoi\"\n",
        "\n",
        "# Step 1: Tokenize text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 2: Convert to lowercase and remove non-alphanumeric tokens\n",
        "tokens = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "# Step 3: Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "# Display the preprocessed tokens\n",
        "print(filtered_tokens)\n",
        "\n",
        "# NLTK is great for beginners; spaCy is better for more advanced tasks."
      ],
      "metadata": {
        "id": "DiGznVN997rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention\n",
        "\n",
        "# Define a single transformer block\n",
        "def transformer_block(input, num_heads, key_dim):\n",
        "    # Step 1: Multi-head attention\n",
        "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(input, input)\n",
        "    # Step 2: Add & Normalize\n",
        "    attention = LayerNormalization()(attention + input)\n",
        "    # Step 3: Feedforward network\n",
        "    dense = Dense(128, activation='relu')(attention)\n",
        "    # Step 4: Add & Normalize\n",
        "    output = LayerNormalization()(dense + attention)\n",
        "    return output\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(None, 128))  # Sequence length is variable, feature size is 128\n",
        "# Transformer block\n",
        "transformer_output = transformer_block(input_layer, num_heads=8, key_dim=64)\n",
        "# Model definition\n",
        "model = tf.keras.Model(inputs=input_layer, outputs=transformer_output)\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8QNN2M8s-KBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine Tune GPT\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Step 2: Prepare the dataset (replace `my_dataset` with your dataset)\n",
        "# Ensure `my_dataset` is a PyTorch dataset with tokenized input\n",
        "# Example tokenization\n",
        "texts = [\"Hello, how are you?\", \"Fine-tuning is fun!\"]\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "labels = encodings.input_ids.clone()\n",
        "\n",
        "my_dataset = torch.utils.data.TensorDataset(encodings.input_ids, labels)\n",
        "\n",
        "# Step 3: Define training arguments\n",
        "train_args = TrainingArguments(\n",
        "    output_dir='./results',        # Directory to save the model\n",
        "    per_device_train_batch_size=4, # Batch size\n",
        "    num_train_epochs=1,            # Number of epochs\n",
        "    save_steps=10_000,             # Steps to save checkpoints\n",
        "    save_total_limit=2,            # Maximum number of saved checkpoints\n",
        "    logging_dir='./logs',          # Directory for logs\n",
        "    logging_steps=500,             # Log every 500 steps\n",
        ")\n",
        "\n",
        "# Step 4: Create Trainer and train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "tp8ijcJL-epz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deployment\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize Flask app and model pipeline\n",
        "app = Flask(__name__)\n",
        "model = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Define API endpoint\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    # Get input text from the request\n",
        "    input_text = request.json[\"input\"]\n",
        "    # Generate text using the model\n",
        "    output = model(input_text, max_length=50)\n",
        "    # Return the generated text as a JSON response\n",
        "    return jsonify(output)\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "Eg8ZYdgu-q4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}